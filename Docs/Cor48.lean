import VersoManual
import Docs.Papers

open Verso.Genre Manual
open Verso.Genre.Manual.InlineLean hiding module
open Verso.Code.External
open Docs

set_option pp.rawOnError true
set_option verso.exampleProject "../axioms_of_adaptivity"
set_option verso.exampleModule "AxiomsOfAdaptivity.EstimatorConvergence"
set_option maxHeartbeats 20000000

#doc (Manual) "Estimator Convergence" =>
%%%
htmlSplit := .never
tag := "estimator_convergence"
%%%

This chapter formalizes the proof of Corollary 4.8 from *AoA* which states

> *Corollary 4.8*: Suppose we know a-priori convergence to some limit $`U_âˆ`
  $$`
  \lim_{l \to \infty} ğ••[\mathcal{T}_l; U_\infty, U(\mathcal{T}_l)] = 0
  `
  and have estimator reduction (for example from {ref "estimator_reduction"}[Lemma 4.7])
  $$`
  Î·(\mathcal{T}_{l+1}; U(\mathcal{T}_{l+1}))Â² â‰¤ Ï_{\mathrm{est}} Î·(\mathcal{T}_l; U(\mathcal{T}_l))Â² + C_{\mathrm{est}} ğ••[\mathcal{T}_{l+1}; U(\mathcal{T}_{l+1}), U(\mathcal{T}_l)]Â².
  `
  This implies the convergence of the estimator $`
  \lim_{l \to \infty} Î·^2(\mathcal{T}_l, U(\mathcal{T}_l)) = 0
  ` and therefore with reliability that $`
  \lim_{l \to \infty} ğ••(\mathcal{T}_l, u, U(\mathcal{T}_l)) = 0.
  `

# Formal statement

We fix the type variables
```anchor vars
variable {Î± Î² : Type*} [DecidableEq Î±] [Lattice Î±] [OrderBot Î±] (alg : AdaptiveAlgorithm Î± Î²)
```
globally and define as a convenient abbreviation
```anchor d_seq
def d_seq n := alg.d (alg.ğ’¯ <| n + 1) (alg.U <| alg.ğ’¯ <| n + 1) (alg.U <| alg.ğ’¯ n)
```

Corollary 4.8 contains two different convergence statement. We split these
into two Lean theorems. The "stronger" theorem we want to show is
```
theorem convergence_of_apriori (hd_seq_lim : Tendsto (d_seq alg) atTop (ğ“ 0)) :
  Tendsto (fun n â†¦ alg.d (alg.ğ’¯ <| n) alg.u (alg.U <| alg.ğ’¯ n)) atTop (ğ“ 0) := by sorry
```
which means that $`ğ••(\mathcal{T}_l, u, U(\mathcal{T}_l))` converges to zero if
we have $`\lim_{l \to \infty} ğ••[\mathcal{T}_l; U(\mathcal{T}_{l+1}), U(\mathcal{T}_l)] = 0`.

Note that this is not exactly the statement from *AoA*. We have left out the implication
$$`
\lim_{l \to \infty} ğ••[\mathcal{T}_l; U_\infty, U(\mathcal{T}_l)] = 0 \Longrightarrow
\lim_{l \to \infty} ğ••[\mathcal{T}_l; U(\mathcal{T}_{l+1}), U(\mathcal{T}_l)] = 0.
`

We will reach this theorem by first showing the intermediate result
```
lemma convergence_of_estimator (hd_seq_lim : Tendsto (d_seq alg) atTop (ğ“ 0)) :
    Tendsto alg.gÎ·2_seq atTop (ğ“ 0) := by { ... }
```
saying that $`Î·^2(\mathcal{T}_l, U(\mathcal{T}_l))` converges to zero given a-priori convergence.
This way, both results from Corollary 4.8 are proven in Lean.

# Proof

Because this proof was the first to be formalized, its structure
is not optimal. It is split into a simple part where the
global error $`Î·^2` and the distance $`ğ••` are replaced by non-negative
sequences and a "glueing" theorem that uses the simpler result to show
estimator convergence for an arbitrary {anchorTerm vars}`AdaptiveAlgorithm`.
The main difference of the stronger variant is
that the codomain of the involved functions (`Î·`, `ğ••`) is `â„` instead of `NNReal`
which was used in the simple part.

## Simple Estimator reduction

In this section $`(Î·_n)` and $`(d_n)` will be non-negative sequences. This clashes
with the notation for the global error and distance, but because the result we will
prove is only useful when we plug in in the appropriate error estimator and distance sequences,
choosing different notation would be confusing.

We begin by defining the simplified assumptions as a structure. In the same vein
as with `AdaptiveAlgorithm`, this is a convenient way to work with the
existential quanitification of multiple constants.
```anchor SimpleEstimatorReduction
structure SimpleEstimatorReduction (Î· d : â„• â†’ NNReal) where
  q : NNReal
  q_range : q âˆˆ Set.Ioo 0 1
  C : NNReal
  C_pos : C > 0
  bound : âˆ€ n, (Î· (n + 1))^2 â‰¤ q * (Î· n)^2 + C * (d n)^2
```
This structure models the assumption of estimator reduction.

All definitions and theorems of this section will be inside the
`SimpleEstimatorReduction` namespace and include an instance of {anchorTerm SimpleEstimatorReduction}`SimpleEstimatorReduction`
as an assumption:
```anchor SimpleEstimatorReduction_preamble
namespace SimpleEstimatorReduction

variable {Î· d : â„• â†’ NNReal} (h : SimpleEstimatorReduction Î· d)
include h
```

For convenience we define the following abbreviations for terms that appear in the
proofs of this section.
```anchor SimpleEstimatorReduction_defs
def weightedSum (n : â„•) : NNReal :=
  âˆ‘ k âˆˆ Finset.range (n + 1), h.q ^ (n - k) * (d k)^2

def upperBound (n : â„•) : NNReal :=
  h.q ^ (n + 1) * (Î· 0)^2 + h.C * h.weightedSum n
```
The finite sum ranges up to $`n`, because the
{anchorTerm SimpleEstimatorReduction_defs}`Finset.range` function gives
the natural numbers less than its argument.

Note that these definitions depend on the constants from the reduction property, which is
possible because of the variable definition from before.
Since we made the definitions in the namespace `SimpleEstimatorReduction` we can then access e.g. the `upperBound` for an instance `h : SimpleEstimatorReduction`
as `h.upperBound` (dot notation).

The goal is to show that our assumptions and
$`\lim_{nâ†’âˆ} d_n = 0` imply that $`\lim_{nâ†’âˆ} Î·_n^2 = 0`. In Lean this
is written as
```
theorem convergence_of_estimator_simple (hd_lim : Tendsto d atTop (ğ“ 0)) : Tendsto (Î·^2) atTop (ğ“ 0) := by sorry
```
where of course we have the included assumption `(h : SimpleEstimatorReduction Î· d)` from the
`variable` statement. In the next sections we will outline the proof of this theorem.

### Upper bound of Estimator

We start by showing that
$$`âˆ€ nâˆˆâ„•:\quad Î·_{n+1}^2 â‰¤ q^{n+1} Î·_0^2 + C âˆ‘_{k=0}^{n} q^{n-k} d_k^2`
by induction. The case $`n=0` is trivial, and the step is shown by
$$`
\begin{aligned}
Î·_{n+2}^2 &â‰¤ q Î·_{n+1}^2 + C d_{n+1}^2 \\
&\stackrel{(IH)}{â‰¤} q \left( q^{n+1} Î·_0^2 + C âˆ‘_{k=0}^n q^{n-k} d_k^2 \right) + C d_{n+1}^2 \\
&= q^{n+2} Î·_0^2 + C âˆ‘_{k=0}^n q^{n-k+1} d_k^2 + C d_{n+1}^2 \\
&= q^{n+2} Î·_0^2 + C âˆ‘_{k=0}^{n+1} q^{n-k+1} d_k^2
\end{aligned}
`

The Lean proof is very much identical, however the last equality
could be more efficient by using automated tactics in combination with
more granular calculation steps. Doing everything at once requires
surgical rewrites.
```anchor estimator_recursive_upper_bound
lemma estimator_recursive_upper_bound (n : â„•) :
    (Î· (n+1))^2 â‰¤ h.upperBound n := by
  induction' n with n ih
  Â· unfold upperBound weightedSum
    simp
    apply h.bound 0
  Â· calc  Î· (n + 1 + 1) ^ 2
      _ â‰¤ h.q * (Î· (n + 1))^2 + h.C * (d (n + 1))^2 := by apply h.bound
      _ â‰¤ h.q * h.upperBound n + h.C * (d (n + 1))^2 := by gcongr
      _ = h.upperBound (n+1) := by
        unfold upperBound weightedSum
        nth_rw 2 [sum_range_succ]
        rw [mul_add, â† mul_assoc, â† pow_succ', â† mul_assoc,
            mul_comm h.q h.C, mul_assoc, mul_sum, mul_add]
        rw [Finset.sum_congr rfl fun k hk => by
          rw [â† mul_assoc, â† pow_succ',
              â† Nat.sub_add_comm (mem_range_succ_iff.mp hk)]]
        simp [pow_zero, add_assoc]
```

### Upper Bound on Weighted Sum

Next, we show that
$$`
âˆ‘_{k=0}^n q^{n-k} d_k^2 â‰¤ (\sup_{i âˆˆ â„•_0} d_i)^2 \frac{qâ»Â¹}{qâ»Â¹ - 1}
`
assuming that $`(d_n)` is bounded above. This is clear from the calculation
$$`
\begin{aligned}
âˆ‘_{k=0}^n q^{n-k} d_k^2 &â‰¤ âˆ‘_{k=0}^n q^{n-k} (\sup_{i âˆˆ â„•_0} d_i)^2 \\
&= (\sup_{i âˆˆ â„•_0} d_i)^2 âˆ‘_{k=0}^n q^{n-k} \\
&= (\sup_{i âˆˆ â„•_0} d_i)^2 q^n âˆ‘_{k=0}^n \frac{1}{q^k} \\
&= (\sup_{i âˆˆ â„•_0} d_i)^2 q^n \frac{(1/q)^{n+1}-1}{1/q - 1} \\
&= (\sup_{i âˆˆ â„•_0} d_i)^2 \frac{1/q - q^n}{1/q - 1} \\
&â‰¤ (\sup_{i âˆˆ â„•_0} d_i)^2 \frac{1/q}{1/q - 1}
\end{aligned}
`
where the essential steps are that we recognise the finite geometric sum and that
we use the bounds $`0 < q < 1`.

The Lean proof uses the same steps, showing supporting results that are used
in rewrites first.
```anchor weighted_sum_bound
lemma weighted_sum_bound (hd : BddAbove (Set.range d)) (n : â„•):
    h.weightedSum n â‰¤ (â¨† i, d i)^2 * (1/h.q) / (1/h.q - 1) := by
  let âŸ¨q, q_range, C, C_pos, boundâŸ© := h
  unfold weightedSum

  have hqâ‚ : 1/q â‰¥ 1 := by
    simp
    apply one_le_inv_iffâ‚€.mpr
    exact âŸ¨q_range.1, le_of_lt q_range.2âŸ©
  have hqâ‚‚ : (1 / q) ^ (n + 1) â‰¥ 1 := one_le_powâ‚€ hqâ‚

  have hâ‚ : âˆ€ k, d k â‰¤ (â¨† i, d i) := by
    intros k
    exact (le_ciSup_iff' hd).mpr fun b a â†¦ a k

  have hâ‚‚ : âˆ‘ k âˆˆ (range (n + 1)), q^(n-k) = âˆ‘ k âˆˆ (range (n + 1)), q^n/q^k := by
    apply Finset.sum_congr rfl
    intros k hk
    rw [â† NNReal.rpow_natCast]
    rw [Nat.cast_sub (mem_range_succ_iff.mp hk)]
    rw [NNReal.rpow_sub_natCast (ne_of_gt q_range.1)]
    simp

  have hâ‚ƒ : âˆ‘ k âˆˆ range (n + 1), (1/q)^k = ((1/q)^(n+1) - 1)/(1/q - 1) := by
    rw[â† NNReal.coe_inj]
    push_cast [hqâ‚, hqâ‚‚]
    apply geom_sum_eq
    Â· simp [ne_of_lt q_range.2]

  have hâ‚„ : q^n * (1/q^(n+1) - 1)/(1/q - 1) = ((1/q) - q^n)/(1/q - 1) := by
    rw [mul_tsub, mul_one, one_div]
    group
    rw [â† zpow_addâ‚€ (ne_of_gt q_range.1)]
    simp

  have hâ‚… : (1/q) - q^n â‰¤ 1/q := by
    have : q^n â‰¤ 1/q := by
      trans 1
      Â· exact pow_le_oneâ‚€ (le_of_lt q_range.1) (le_of_lt q_range.2)
      Â· exact hqâ‚
    rw [â† NNReal.coe_le_coe]
    push_cast [this]
    simp

  calc âˆ‘ k âˆˆ (range (n + 1)), q^(n-k) * (d k)^2
    _ â‰¤ âˆ‘ k âˆˆ (range (n + 1)), q^(n-k) * (â¨† i, d i)^2 := by gcongr; apply hâ‚
    _ = âˆ‘ k âˆˆ (range (n + 1)), (â¨† i, d i)^2 * q^(n-k) := by simp_rw [mul_comm]
    _ = (â¨† i, d i)^2 * âˆ‘ k âˆˆ range (n + 1), q^(n-k) := by simp [mul_sum]
    _ = (â¨† i, d i)^2 * âˆ‘ k âˆˆ range (n + 1), q^n/q^k := by rw [hâ‚‚]
    _ = (â¨† i, d i)^2 * âˆ‘ k âˆˆ range (n + 1), q^n * (1/q)^k := by field_simp
    _ = (â¨† i, d i)^2 * q^n * âˆ‘ k âˆˆ range (n + 1), (1/q)^k := by simp [â† mul_sum, mul_assoc]
    _ = (â¨† i, d i)^2 * (q^n * (1/q^(n+1) - 1)/(1/q - 1)) := by rw [hâ‚ƒ]; field_simp [mul_assoc]
    _ = (â¨† i, d i)^2 * ((1/q) - q^n)/(1/q - 1) := by rw [hâ‚„, â† mul_div_assoc']
    _ â‰¤ (â¨† i, d i)^2 * (1/q)/(1/q - 1) := by gcongr
```
In {anchorTerm weighted_sum_bound}`hâ‚ƒ` we use the geometric sum theorem from mathlib,
which assumes more structure than `NNReal` has. Therefore we have to cast
to the reals and push the cast inwards. For this we have to supply
proof that the terms involved are non-negative ({anchorTerm weighted_sum_bound}`hqâ‚`,
{anchorTerm weighted_sum_bound}`hqâ‚‚`).

### Boundedness of Î·
%%%
tag := "boundedness_eta"
%%%

The main $`d` argument for $`\lim_{nâ†’âˆ} Î·_n = 0` uses the $`\limsup` of $`(Î·_n)`.
Because the $`\limsup` of an unbounded sequence is defined to be zero
in Lean, the next step will be to explicitly show that $`(Î·_n)`
is bounded, giving us access to mathlib theorems about $`\limsup`.

We show that $`(Î·_n)` is bounded above by $`\sqrt{K}` where
$$`
K \coloneqq \max \{ Î·_0^2 + C (\sup_{i âˆˆ â„•_0} d_i)^2 \frac{\frac{1}{q}}{\frac{1}{q} - 1},\; Î·_0^2 \}
` (of course still assuming that $`(d_n)` is bounded).
Using the maximum here is mathematically non-sensical because
the first value is greater or equal than the second one. In Lean
it avoids having to show non-negativity of the
$`C (\sup_{i âˆˆ â„•_0} d_i)^2 \frac{\frac{1}{q}}{\frac{1}{q} - 1}` term.

What is left to show after taking the square is that $`Î·_n^2 â‰¤ K`.
We make a case distinction. If $`n=0`, because of the maximum in the
definition of $`K`, the estimate is trivial. Otherwise, $`n-1` is still
a natural number and:
$$`
\begin{aligned}
Î·_n^2 &= Î·_{(n-1)+1}^2 \\
&\stackrel{(\mathrm{Estimator\ Bound})}{â‰¤} q^{n} Î·_0^2 + C âˆ‘_{k=0}^{n-1} q^{n-1-k} d_k^2 \\
&\stackrel{(\mathrm{Sum\ Bound})}{â‰¤} q^{n} Î·_0^2 + C (\sup_{i âˆˆ â„•_0} d_i)^2 \frac{qâ»Â¹}{qâ»Â¹ - 1}
&â‰¤ Î·_0^2 + C (\sup_{i âˆˆ â„•_0} d_i)^2 \frac{qâ»Â¹}{qâ»Â¹ - 1}
\end{aligned}
`

The Lean proof mirrors this argument:
```anchor estimator_bounded
lemma estimator_bounded (hd : BddAbove (Set.range d)) : BddAbove (Set.range Î·) := by
  let K := ((Î· 0)^2 + h.C * ((â¨† i, d i)^2 * (1/h.q)/(1/h.q - 1))) âŠ” ((Î· 0)^2)
  use NNReal.sqrt K

  intros x hx
  rcases hx with âŸ¨n,hnâŸ©
  rw [â† hn]

  apply NNReal.le_sqrt_iff_sq_le.mpr
  by_cases hn : n = 0
  case pos =>
    unfold K
    rw [hn]
    apply le_max_right
  case neg =>
    have : n-1+1 = n := Nat.succ_pred_eq_of_ne_zero hn
    calc (Î· n)^2
      _ = (Î· ((n-1)+1))^2 := by rw [this]
      _ â‰¤ h.upperBound (n-1) := by exact estimator_recursive_upper_bound h (n-1)
      _ = h.q^n * (Î· 0)^2 + h.C * h.weightedSum (n-1) := by {unfold upperBound; simp [this]}
      _ â‰¤ h.q^n * (Î· 0)^2 + h.C * ((â¨† i, d i)^2 * (1/h.q)/(1/h.q - 1)) := by rel [weighted_sum_bound h hd (n-1)]
      _ â‰¤ (Î· 0)^2 + h.C * ((â¨† i, d i)^2 * (1/h.q)/(1/h.q - 1)) := by
        gcongr
        by_cases hÎ· : (Î· 0)^2 = 0
        case pos =>
          simp [hÎ·]
        case neg =>
          have : h.q^n â‰¤ 1 := pow_le_one' (le_of_lt h.q_range.2) n
          rw [â† mul_le_mul_right (pos_of_ne_zero hÎ·)] at this
          simpa using this
      _ â‰¤ K := by unfold K; apply le_max_left
```

### Limsup of Î· is Zero

Now we can show that $`\limsup_{nâ†’âˆ} Î·_n = 0` assuming $`\lim_{nâ†’âˆ} d_n = 0` and boundedness
of $`Î·`.
We do this with the help of the utility lemma
```
lemma smaller_q_eq_zero (a q: NNReal) (hq : q < 1) (ha : a â‰¤ q*a) : a = 0 := by sorry
```
whose proof we will skip. It is available in the {ref "code"}[Lean code].

So, with the constant $`q` from the estimator reduction assumption,
it suffices to show
$$`
\limsup_{nâ†’âˆ} Î·_n â‰¤ q \limsup_{nâ†’âˆ} Î·_n.
`

This is clear from
$$`
\begin{aligned}
\limsup_{n \to \infty} Î·_n^2 &= \limsup_{n \to \infty} Î·_{n+1}^2 \\
&â‰¤ \limsup_{n \to \infty} (q Î·_n^2 + C d_n^2) \\
&â‰¤ \limsup_{n \to \infty} q Î·_n^2 + \underbrace{\limsup_{n \to \infty} C d_n^2}_{=0\ (\mathrm{convergence\ of\ }\ d_n)} \\
&= \limsup_{n \to \infty} q Î·_n^2 \\
&= q \limsup_{n \to \infty} Î·_n^2
\end{aligned}
`
using the convergence of $`(d_n)` and boundedness of $`(Î·_n)`. The Lean proof
follows this idea and uses utility theorems to supply all necessary boundedness
proofs.

```anchor estimator_limsup_zero
lemma estimator_limsup_zero (hd : Tendsto d atTop (ğ“ 0)) (hÎ·â‚ : BddAbove (Set.range Î·)) :
    limsup (Î·^2) atTop = 0 := by
  let âŸ¨q, q_range, C, C_pos, boundâŸ© := h

  apply smaller_q_eq_zero _ q q_range.2

  have hdc : Tendsto (C â€¢ d^2) atTop (ğ“ 0) := by
    have := Filter.Tendsto.pow hd 2
    have := Filter.Tendsto.mul_const C this
    simpa [mul_comm] using this

  have hÎ·â‚‚ : BddAbove (Set.range (Î·^2)) := monotone_map_bdd_above_range (pow_left_mono 2) hÎ·â‚
  have hÎ·â‚ƒ : BddAbove (Set.range (q â€¢ Î·^2)) := monotone_map_bdd_above_range mul_left_mono hÎ·â‚‚

  have hâ‚ : limsup ((Î·^2) âˆ˜ (Â· + 1)) atTop â‰¤ limsup (q â€¢ Î·^2 + C â€¢ d^2) atTop := by
    apply Filter.limsup_le_limsup
    Â· exact Filter.Eventually.of_forall bound
    Â· apply Filter.IsBoundedUnder.isCoboundedUnder_le
      apply BddBelow.isBoundedUnder_of_range
      apply nnreal_fun_bbd_below
    Â· apply BddAbove.isBoundedUnder_of_range
      apply BddAbove.range_add hÎ·â‚ƒ <| Tendsto.bddAbove_range hdc

  have hâ‚‚ : limsup (q â€¢ Î·^2 + C â€¢ d^2) atTop â‰¤ limsup (q â€¢ Î·^2) atTop + limsup (C â€¢ d^2) atTop := by
    rw [â† NNReal.coe_le_coe]
    push_cast [â† NNReal.toReal_limsup]

    apply limsup_add_le ?cÎ·_below ?cÎ·_above ?cd_below ?cd_above
    case cÎ·_below =>
      exact BddBelow.isBoundedUnder_of_range <| lift_bound_below _
    case cÎ·_above =>
      exact BddAbove.isBoundedUnder_of_range <| lift_bound_above _ hÎ·â‚ƒ
    case cd_below =>
      exact Filter.IsBoundedUnder.isCoboundedUnder_le <| BddBelow.isBoundedUnder_of_range <| lift_bound_below _
    case cd_above =>
      exact BddAbove.isBoundedUnder_of_range <| lift_bound_above _ <| Tendsto.bddAbove_range hdc

  calc limsup (Î·^2) atTop
    _ = limsup (Î» n â†¦ (Î· (n+1))^2) atTop := by rw [â† Filter.limsup_nat_add _ 1]; rfl
    _ = limsup ((Î·^2) âˆ˜ (Â· + 1)) atTop := by rfl
    _ â‰¤ limsup (q â€¢ Î·^2 + C â€¢ d^2) atTop := by exact hâ‚
    _ â‰¤ limsup (q â€¢ Î·^2) atTop + limsup (C â€¢ d^2) atTop := by exact hâ‚‚
    _ = limsup (q â€¢ Î·^2) atTop := by simp [Tendsto.limsup_eq hdc]
    _ = q * limsup (Î·^2) atTop := by exact nnreal_limsup_const_mul <| BddAbove.isBoundedUnder_of_range hÎ·â‚‚
```
The boundedness proofs are necessary to apply mathlib theorems about `limsup` and use the result
from the {ref "boundedness_eta"}[previous section]. Also note that `â€¢` is the pointwise
multiplication in Lean and is used in the proof multiple times to avoid writing the argument of
{anchorTerm estimator_limsup_zero}`limsup`
as an anonymous function.

### Convergence of Î· to Zero

The final step is to conclude convergence of $`(Î·_n)` . We already know that
$`\limsup_{nâ†’âˆ} Î·_n = 0`. Naturally
$$`
\liminf_{nâ†’âˆ} Î·_n â‰¤ \limsup_{nâ†’âˆ} Î·_n = 0.
`
So by standard analysis, if $`\liminf` and $`\limsup` agree, we have
convergence, which means $`\lim_{nâ†’âˆ} Î·_n = 0`.

The Lean proof is totally analogous, again supplying additional boundedness
proofs to unlock the analytical mathlib theorems
```anchor convergence_of_estimator_simple
theorem convergence_of_estimator_simple (hd_lim : Tendsto d atTop (ğ“ 0)) : Tendsto (Î·^2) atTop (ğ“ 0) := by
  let hd_above := Tendsto.bddAbove_range hd_lim
  let hÎ·_above := estimator_bounded h hd_above
  have hÎ·2_above := monotone_map_bdd_above_range (pow_left_mono 2) hÎ·_above
  have hÎ·2_below : BddBelow (Set.range (Î·^2)) := nnreal_fun_bbd_below _
  let hÎ·_limsup := estimator_limsup_zero h hd_lim hÎ·_above

  apply tendsto_of_liminf_eq_limsup
  case hinf =>
    apply nonpos_iff_eq_zero.mp
    rw [â† hÎ·_limsup]
    apply liminf_le_limsup
    Â· exact BddAbove.isBoundedUnder_of_range hÎ·2_above
    Â· exact BddBelow.isBoundedUnder_of_range hÎ·2_below
  case hsup => exact hÎ·_limsup
  case h => exact BddAbove.isBoundedUnder_of_range hÎ·2_above
  case h' => exact BddBelow.isBoundedUnder_of_range hÎ·2_below
```
Now we have reached the final conclusion of `SimpleEstimatorReduction`.

## Estimator Convergence for `AdaptiveAlgorithm`

In a "glueing" theorem we can now use the theory of `SimpleEstimatorReduction`
to show the actual statement of Corollary 4.8. The first step is
to port the result of `SimpleEstimatorReduction` to the `AdaptiveAlgorithm`
world, i.e. we show that $`\lim_{lâ†’âˆ} Î·^2(\mathcal{T}_l, U(\mathcal{T}_l)) = 0`
for the actual refinement indicator `Î·`.

The only non-trivial step in this endeavour is that
the estimator reduction in {ref "lemma47_formal_statement"}[Lemma 4.7]
was formulated
for any $`Î´ > 0` with $`Ï_{\mathrm{est}}(Î´) < 1`. So, for an estimator reduction property
to actually hold, we have to find a concrete such $`Î´`. This is done
in the utility lemma
```
lemma estimator_reduction_delta_exists : âˆƒ Î´ > 0, alg.Ï_est Î´ âˆˆ Set.Ioo 0 1 âˆ§ 0 < alg.C_est Î´ := by sorry
```
which is has an uninspiring proof of the fact that
$$`
Î´ := \frac12 ((1 - Ï_{\mathrm{red}}) Î¸ (1 - (1 - Ï_{\mathrm{red}}) * Î¸)â»Â¹)
`
fulfils $`Ï_{\mathrm{est}}(Î´) < 1`.

Apart from that, mathematically speaking,
it is very obvious that the simplified theorem applies to
the sequences generated from the `AdaptiveAlgorithm`. However, in
Lean this requires a few lines of code. Especially the conversion
between sequences in the `NNReal`s and real sequences requires some
extra proofs:

```anchor convergence_of_estimator
lemma convergence_of_estimator (hd_seq_lim : Tendsto (d_seq alg) atTop (ğ“ 0)) :
    Tendsto alg.gÎ·2_seq atTop (ğ“ 0) := by

  -- first define the object we want to apply the simplified convergence
  -- theorem to
  rcases alg.estimator_reduction_delta_exists with âŸ¨Î´, hÎ´, âŸ¨hÏ_est, hC_estâŸ©âŸ©

  let Ï_est := alg.Ï_est Î´
  let C_est := alg.C_est Î´

  have estimator_reduction := alg.estimator_reduction Î´ hÎ´ hÏ_est.2

  let d n := (d_seq alg n).toNNReal

  let est_red := {
    q := Ï_est.toNNReal,
    C := C_est.toNNReal,
    C_pos := by simpa using hC_est
    q_range := by simpa using hÏ_est
    bound := by {
      intros n
      apply NNReal.coe_le_coe.mp
      push_cast

      have hd : d n = d_seq alg n := by
        apply Real.coe_toNNReal
        apply alg.non_neg

      have hq : Ï_est.toNNReal = Ï_est := by
        apply Real.coe_toNNReal
        exact le_of_lt hÏ_est.1

      have hC : C_est.toNNReal = C_est := by
        apply Real.coe_toNNReal
        exact le_of_lt hC_est

      simp only [alg.hnn_gÎ·_seq, hd, hq, hC]
      unfold d_seq
      exact estimator_reduction n
    }
  : SimpleEstimatorReduction alg.nn_gÎ·_seq d}

  have hd_lim : Tendsto d atTop (ğ“ 0) := by
    rw [Eq.symm Real.toNNReal_zero]
    apply tendsto_real_toNNReal hd_seq_lim

  conv =>
    enter [1, n]
    rw [â† alg.hnn_gÎ·_seq n]
    norm_cast
  rw [â† NNReal.coe_zero]
  apply NNReal.tendsto_coe.mpr
  exact est_red.convergence_of_estimator_simple hd_lim
```
The main point here is that we define the instance {anchorTerm convergence_of_estimator}`est_red`
of type {anchorTerm convergence_of_estimator}`SimpleEstimatorReduction` and access its
{anchorTerm convergence_of_estimator}`est_red.convergence_of_estimator_simple` proof
to show the claim. The sequence we use for $`(Î·_n)` is {anchorTerm convergence_of_estimator}`nn_gÎ·_seq`
from the {ref "adaptive_alg_defs"}[`AdaptiveAlgorithm` definitions].

Now, the final step is to show convergence of the distance to the unkown limit $`u`.
This follows from reliability, because it allows us to
sandwich $`(ğ••(\mathcal{T}_l, u, U(\mathcal{T}_l)))_{lâˆˆâ„•}`
between the zero-convergent sequence $`(\sqrt{Î·^2(\mathcal{T}_l, U(\mathcal{T}_l))})_{lâˆˆâ„•}` and the constant
zero sequence:
$$`
0 â‰¤ ğ••(\mathcal{T}_l, u, U(\mathcal{T}_l)) â‰¤ C_{\mathrm{rel}} \sqrt{Î·^2(\mathcal{T}_l, U(\mathcal{T}_l))}
`
This is translates nicely to a Lean proof using the {anchorTerm convergence_of_apriori}`squeeze_zero`
theorem from mathlib.
```anchor convergence_of_apriori
theorem convergence_of_apriori (hd_seq_lim : Tendsto (d_seq alg) atTop (ğ“ 0)) :
  Tendsto (fun n â†¦ alg.d (alg.ğ’¯ <| n) alg.u (alg.U <| alg.ğ’¯ n)) atTop (ğ“ 0) := by
    have := Filter.Tendsto.sqrt (convergence_of_estimator alg hd_seq_lim)
    have := Filter.Tendsto.const_mul alg.C_rel this
    simp at this

    apply squeeze_zero _ _ this
    Â· exact fun _ â†¦ by apply alg.non_neg
    Â· intros t
      apply alg.reliability
```
This concludes the Lean proof of Corollary 4.8
