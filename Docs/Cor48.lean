import VersoManual
import Docs.Papers

open Verso.Genre Manual
open Verso.Genre.Manual.InlineLean
open Verso.Code.External
open Docs

set_option pp.rawOnError true
set_option verso.exampleProject "../axioms_of_adaptivity"
set_option verso.exampleModule "AxiomsOfAdaptivity.EstimatorConvergence"
set_option maxHeartbeats 20000000

#doc (Manual) "Estimator Convergence" =>
%%%
htmlSplit := .never
%%%

This chapter formalizes the proof of Corollary 4.8 from *AoA* which states

> *Corollary 4.8*: Suppose we know a-priori convergence to some limit $`U_âˆ`
  $$`
  \lim_{l \to \infty} \mathbb{d}[\mathcal{T}_l; U_\infty, U(\mathcal{T}_l)] = 0
  `
  and have estimator reduction (for example from {ref "estimator_reduction"}[Lemma 4.7])
  $$`
  Î·(ğ“£_{â„“+1}; U(ğ“£_{â„“+1}))Â² â‰¤ Ï_{est} Î·(ğ“£_â„“; U(ğ“£_â„“))Â² + C_{est} ğ••[ğ“£_{â„“+1}; U(ğ“£_{â„“+1}), U(ğ“£_â„“)]Â².
  `
  This implies the convergence of the estimator $`
  \lim_{l \to \infty} Î·^2(ğ’¯_l, U(ğ’¯_l)) = 0
  ` and therefore with reliability that $`
  \lim_{l \to \infty} \mathbb{d}(ğ’¯_l, u, U(ğ’¯_l)) = 0.
  `

# Formal statement

For the following variables
```anchor vars
variable {Î± Î² : Type*} [DecidableEq Î±] [Lattice Î±] [OrderBot Î±] (alg : AdaptiveAlgorithm Î± Î²)
```
we define as a convenient abbreviation
```anchor d_seq
def d_seq n := alg.d (alg.ğ’¯ <| n + 1) (alg.U <| alg.ğ’¯ <| n + 1) (alg.U <| alg.ğ’¯ n)
```

Corollary 4.8 mentions two different convergences. We split these
into two Lean theorems. The "larger" theorem we want to ultimately show is
```
theorem convergence_of_apriori (hd_seq_lim : Tendsto (d_seq alg) atTop (ğ“ 0)) :
  Tendsto (fun n â†¦ alg.d (alg.ğ’¯ <| n) alg.u (alg.U <| alg.ğ’¯ n)) atTop (ğ“ 0) := by { ... }
```
which means that $`\mathbb{d}(ğ’¯_l, u, U(ğ’¯_l))` converges to zero if
we have $`\lim_{l \to \infty} \mathbb{d}[\mathcal{T}_l; U(\mathcal{T}_{l+1}), U(\mathcal{T}_l)] = 0`.
Note that this is not exactly the statement from *AoA*. We have left out the implication
$$`
\lim_{l \to \infty} \mathbb{d}[\mathcal{T}_l; U_\infty, U(\mathcal{T}_l)] = 0 \Longrightarrow
\lim_{l \to \infty} \mathbb{d}[\mathcal{T}_l; U(\mathcal{T}_{l+1}), U(\mathcal{T}_l)] = 0.
`
-- TODO what about this implication??

We will reach this theorem by first showing the intermediate result
```
lemma convergence_of_estimator (hd_seq_lim : Tendsto (d_seq alg) atTop (ğ“ 0)) :
    Tendsto alg.gÎ·2_seq atTop (ğ“ 0) := by { ... }
```
saying that $`Î·^2(ğ’¯_l, U(ğ’¯_l))` converges to zero given a-priori convergence.
This way, both implications from Corollary 4.8 are proven in Lean.

# Proof

Due to this proof being the first one to be formalized, its structure
is not optimal. It is split into a simple part where the
global error $`Î·` and the distance $`\mathbb{d}` are replaced by non-negative
sequences and a bridging theorem that uses the simpler result to show
estimator convergence for an arbitrary $`AdaptiveAlgorithm`, the main difference
being that the codomain of the involved functions (`Î·`, `d`) is `â„` instead of `NNReal`
which was used in the simple part.

## Simple Estimator reduction

In this section $`(Î·_n)` and $`(d_n)` will be non-negative sequences. This clashes
with the notation for the global error and distance, however because the result we will
prove is only useful when we plugin in the appropriate error estimator and distance sequences
choosing different notation would not be benefitial.

We begin by defining the simplified assumptions as a structure. In the same vein
as with `AdaptiveAlgorithm`, this is a convenient way to work with the
existential quanitification of multiple constants.
```anchor SimpleEstimatorReduction
structure SimpleEstimatorReduction (Î· d : â„• â†’ NNReal) where
  q : NNReal
  q_range : q âˆˆ Set.Ioo 0 1
  C : NNReal
  C_pos : C > 0
  bound : âˆ€ n, (Î· (n + 1))^2 â‰¤ q * (Î· n)^2 + C * (d n)^2
```
This models the assumption of estimator reduction.

All definitions and theorems of this section will be inside the
{anchorTerm SimpleEstimatorReduction_preamble}`SimpleEstimatorReduction` namespace and include an instance of {anchorTerm SimpleEstimatorReduction}`SimpleEstimatorReduction`
as an assumption:
```anchor SimpleEstimatorReduction_preamble
namespace SimpleEstimatorReduction

variable {Î· d : â„• â†’ NNReal} (h : SimpleEstimatorReduction Î· d)
include h
```

For convenience we define the following abbreviations for terms that appear in the
proofs of this section.
```anchor SimpleEstimatorReduction_defs
def weightedSum (n : â„•) : NNReal :=
  âˆ‘ k âˆˆ Finset.range (n + 1), h.q ^ (n - k) * (d k)^2

def upperBound (n : â„•) : NNReal :=
  h.q ^ (n + 1) * (Î· 0)^2 + h.C * h.weightedSum n
```
The finite sum ranges up to $`n`, because the `Finset.range` function gives
the natural numbers less than its argument.
Note that they depend on the constants from the reduction property, which is
possible because of the variable definition from before. Because
of the namespace we can then access the e.g. `upperBound` for an instance `h : SimpleEstimatorReduction`
as `h.upperBound`.

The goal is to show from the assumption
$`\lim_{nâ†’âˆ} d_n = 0` that $`\lim_{nâ†’âˆ} Î·_n^2 = 0`, or in Lean
```
theorem convergence_of_estimator_simple (hd_lim : Tendsto d atTop (ğ“ 0)) : Tendsto (Î·^2) atTop (ğ“ 0) := by sorry
```

### Upper bound of Estimator

We start by showing that
$$`âˆ€ nâˆˆâ„•_0:\quad Î·_{n+1}^2 â‰¤ q^{n+1} Î·_0^2 + C âˆ‘_{k=0}^{n} q^{n-k} d_k^2`
by induction. The case $`n=0` is trivial, and the step is shown by
$$`
\begin{aligned}
Î·_{n+2}^2 &â‰¤ q Î·_{n+1}^2 + C d_{n+1}^2 \\
&\stackrel{(IH)}{â‰¤} q \left( q^{n+1} Î·_0^2 + C âˆ‘_{k=0}^n q^{n-k} d_k^2 \right) + C d_{n+1}^2 \\
&= q^{n+2} Î·_0^2 + C âˆ‘_{k=0}^n q^{n-k+1} d_k^2 + C d_{n+1}^2 \\
&= q^{n+2} Î·_0^2 + C âˆ‘_{k=0}^{n+1} q^{n-k+1} d_k^2
\end{aligned}
`

The Lean proof is very much identical, however the last equality
could be more efficient by using automated tactics in combination with
more granular calculation steps. Doing everything at once requires
surgical rewrites.
```anchor estimator_recursive_upper_bound
lemma estimator_recursive_upper_bound (n : â„•) :
    (Î· (n+1))^2 â‰¤ h.upperBound n := by {
  induction' n with n ih
  Â· unfold upperBound weightedSum
    simp
    apply h.bound 0
  Â· calc  Î· (n + 1 + 1) ^ 2
      _ â‰¤ h.q * (Î· (n + 1))^2 + h.C * (d (n + 1))^2 := by apply h.bound
      _ â‰¤ h.q * h.upperBound n + h.C * (d (n + 1))^2 := by gcongr
      _ = h.upperBound (n+1) := by {
        unfold upperBound weightedSum
        nth_rw 2 [sum_range_succ]
        rw [mul_add, â† mul_assoc, â† pow_succ', â† mul_assoc, mul_comm h.q h.C, mul_assoc, mul_sum, mul_add]
        rw [Finset.sum_congr rfl fun k hk => by
          rw [â† mul_assoc, â† pow_succ', â† Nat.sub_add_comm (mem_range_succ_iff.mp hk)]]
        simp [pow_zero, add_assoc]
      }
}
```

### Upper Bound on Weighted Sum

Next, we show that
$$`
âˆ‘_{k=0}^n q^{n-k} d_k^2 â‰¤ (\sup_{i âˆˆ â„•_0} d_i)^2 \frac{qâ»Â¹}{qâ»Â¹ - 1}
`
assuming that $`(d_n)` is bounded above. This is clear from the calculation
$$`
\begin{aligned}
âˆ‘_{k=0}^n q^{n-k} d_k^2 &â‰¤ âˆ‘_{k=0}^n q^{n-k} (\sup_{i âˆˆ â„•_0} d_i)^2 \\
&= (\sup_{i âˆˆ â„•_0} d_i)^2 âˆ‘_{k=0}^n q^{n-k} \\
&= (\sup_{i âˆˆ â„•_0} d_i)^2 q^n âˆ‘_{k=0}^n \frac{1}{q^k} \\
&= (\sup_{i âˆˆ â„•_0} d_i)^2 q^n \frac{(1/q)^{n+1}-1}{1/q - 1} \\
&= (\sup_{i âˆˆ â„•_0} d_i)^2 \frac{1/q - q^n}{1/q - 1} \\
&â‰¤ (\sup_{i âˆˆ â„•_0} d_i)^2 \frac{1/q}{1/q - 1}
\end{aligned}
`
where the essential steps are that we recognise the finite geometric sum and that
we use the bounds $`0 < q < 1`.

The Lean proof uses the same steps, showing supporting results that can be used
in rewrites first.
```anchor weighted_sum_bound
lemma weighted_sum_bound (hd : BddAbove (Set.range d)) (n : â„•):
    h.weightedSum n â‰¤ (â¨† i, d i)^2 * (1/h.q) / (1/h.q - 1) := by {
  let âŸ¨q, q_range, C, C_pos, boundâŸ© := h
  unfold weightedSum

  have hqâ‚ : 1/q â‰¥ 1 := by {
    simp
    apply one_le_inv_iffâ‚€.mpr
    exact âŸ¨q_range.1, le_of_lt q_range.2âŸ©
  }
  have hqâ‚‚ : (1 / q) ^ (n + 1) â‰¥ 1 := one_le_powâ‚€ hqâ‚

  have hâ‚ : âˆ€ k, d k â‰¤ (â¨† i, d i) := by {
    intros k
    exact (le_ciSup_iff' hd).mpr fun b a â†¦ a k
  }

  have hâ‚‚ : âˆ‘ k âˆˆ (range (n + 1)), q^(n-k) = âˆ‘ k âˆˆ (range (n + 1)), q^n/q^k := by {
    apply Finset.sum_congr rfl
    intros k hk
    rw [â† NNReal.rpow_natCast]
    rw [Nat.cast_sub (mem_range_succ_iff.mp hk)]
    rw [NNReal.rpow_sub_natCast (ne_of_gt q_range.1)]
    simp
  }

  have hâ‚ƒ : âˆ‘ k âˆˆ range (n + 1), (1/q)^k = ((1/q)^(n+1) - 1)/(1/q - 1) := by {
    rw[â† NNReal.coe_inj]
    push_cast [hqâ‚, hqâ‚‚]
    apply geom_sum_eq
    Â· simp [ne_of_lt q_range.2]
  }

  have hâ‚„ : q^n * (1/q^(n+1) - 1)/(1/q - 1) = ((1/q) - q^n)/(1/q - 1) := by {
    rw [mul_tsub, mul_one, one_div]
    group
    rw [â† zpow_addâ‚€ (ne_of_gt q_range.1)]
    simp
  }

  have hâ‚… : (1/q) - q^n â‰¤ 1/q := by {
    have : q^n â‰¤ 1/q := by {
      trans 1
      Â· exact pow_le_oneâ‚€ (le_of_lt q_range.1) (le_of_lt q_range.2)
      Â· exact hqâ‚
    }
    rw [â† NNReal.coe_le_coe]
    push_cast [this]
    simp
  }

  calc âˆ‘ k âˆˆ (range (n + 1)), q^(n-k) * (d k)^2
    _ â‰¤ âˆ‘ k âˆˆ (range (n + 1)), q^(n-k) * (â¨† i, d i)^2 := by gcongr; apply hâ‚
    _ = âˆ‘ k âˆˆ (range (n + 1)), (â¨† i, d i)^2 * q^(n-k) := by simp_rw [mul_comm]
    _ = (â¨† i, d i)^2 * âˆ‘ k âˆˆ range (n + 1), q^(n-k) := by simp [mul_sum]
    _ = (â¨† i, d i)^2 * âˆ‘ k âˆˆ range (n + 1), q^n/q^k := by rw [hâ‚‚]
    _ = (â¨† i, d i)^2 * âˆ‘ k âˆˆ range (n + 1), q^n * (1/q)^k := by field_simp
    _ = (â¨† i, d i)^2 * q^n * âˆ‘ k âˆˆ range (n + 1), (1/q)^k := by simp [â† mul_sum, mul_assoc]
    _ = (â¨† i, d i)^2 * (q^n * (1/q^(n+1) - 1)/(1/q - 1)) := by rw [hâ‚ƒ]; field_simp [mul_assoc]
    _ = (â¨† i, d i)^2 * ((1/q) - q^n)/(1/q - 1) := by rw [hâ‚„, â† mul_div_assoc']
    _ â‰¤ (â¨† i, d i)^2 * (1/q)/(1/q - 1) := by gcongr
}
```
In {anchorTerm weighted_sum_bound}`hâ‚ƒ` we use the geometric sum theorem from mathlib,
which is formulated for the real numbers. Therefore we have to cast
to the reals and push the cast inwards. For this we have to supply
proof that the terms involved are non-negative ({anchorTerm weighted_sum_bound}`hqâ‚`,
{anchorTerm weighted_sum_bound}`hqâ‚‚`).

### Boundedness of Î·

-- TODO unify the "we need this because operators have defaults" stories
The main argument for $`\lim_{nâ†’âˆ} Î·_n = 0` uses the $`\lim\sup` of $`(Î·_n)`.
Because the $`\lim\sup` of an unbounded sequence is defined to be zero
in Lean, the next step will be to explicitly show that $`(Î·_n)`
is bounded, giving us access to mathlib theorems about $`\lim\sup`.

We show that $(Î·_n)$ is bounded above by $`\sqrt{K}` where
$$`
K \coloneqq \max { Î·_0^2 + C (\sup_{i âˆˆ â„•_0} d_i)^2 \frac{1/q}{1/q - 1}, Î·_0^2 }.
` (of course still assuming that $(d_n)$ is bounded).
Using the maximum here is mathematically non-sensical because
the first value is greater or equal than the second one. In Lean
it avoids having to show non-negativity of the
$`C (\sup_{i âˆˆ â„•_0} d_i)^2 \frac{1/q}{1/q - 1}` term.

What is left to show after taking the square is that $`Î·_n^2 â‰¤ K`.
We make a case distinction. If $`n=0`, because of the maximum in the
definition of $`K`, the estimate is trivial. Otherwise, $`n-1` is still
a natural number and:
$$`
\begin{aligned}
Î·_n^2 &= Î·_{(n-1)+1}^2 \\
&\stackrel{(Estimator Bound)}{â‰¤} q^{n} Î·_0^2 + C âˆ‘_{k=0}^{n-1} q^{n-1-k} d_k^2 \\
&\stackrel{(Sum Bound)}{â‰¤} q^{n} Î·_0^2 + C (\sup_{i âˆˆ â„•_0} d_i)^2 \frac{qâ»Â¹}{qâ»Â¹ - 1}
&â‰¤ Î·_0^2 + C (\sup_{i âˆˆ â„•_0} d_i)^2 \frac{qâ»Â¹}{qâ»Â¹ - 1}
\end{aligned}
`

The Lean proof mirrors this argument:
```anchor estimator_bounded
lemma estimator_bounded (hd : BddAbove (Set.range d)) : BddAbove (Set.range Î·) := by {
  let K := ((Î· 0)^2 + h.C * ((â¨† i, d i)^2 * (1/h.q)/(1/h.q - 1))) âŠ” ((Î· 0)^2)
  use NNReal.sqrt K

  intros x hx
  rcases hx with âŸ¨n,hnâŸ©
  rw [â† hn]

  apply NNReal.le_sqrt_iff_sq_le.mpr
  by_cases hn : n = 0
  case pos =>
    unfold K
    rw [hn]
    apply le_max_right
  case neg =>
    have : n-1+1 = n := Nat.succ_pred_eq_of_ne_zero hn
    calc (Î· n)^2
      _ = (Î· ((n-1)+1))^2 := by rw [this]
      _ â‰¤ h.upperBound (n-1) := by exact estimator_recursive_upper_bound h (n-1)
      _ = h.q^n * (Î· 0)^2 + h.C * h.weightedSum (n-1) := by {unfold upperBound; simp [this]}
      _ â‰¤ h.q^n * (Î· 0)^2 + h.C * ((â¨† i, d i)^2 * (1/h.q)/(1/h.q - 1)) := by rel [weighted_sum_bound h hd (n-1)]
      _ â‰¤ (Î· 0)^2 + h.C * ((â¨† i, d i)^2 * (1/h.q)/(1/h.q - 1)) := by {
        gcongr
        by_cases hÎ· : (Î· 0)^2 = 0
        case pos =>
          simp [hÎ·]
        case neg =>
          have : h.q^n â‰¤ 1 := pow_le_one' (le_of_lt h.q_range.2) n
          rw [â† mul_le_mul_right (pos_of_ne_zero hÎ·)] at this
          simpa using this
      }
      _ â‰¤ K := by unfold K; apply le_max_left
}
```

### Limsup of Î· is Zero

Now we can show that assuming $\lim_{nâ†’âˆ} d_n = 0$ and boundedness
of $`Î·` that $`\lim\sup_{nâ†’âˆ} Î·_n = 0`.
We do this with the help of the following lemma whose proof we will skip
```lean
lemma smaller_q_eq_zero (a q: NNReal) (hq : q < 1) (ha : a â‰¤ q*a) : a = 0 := by sorry
```

So using the $`q` from the estimator reduction assumption,
it suffices to show
$$`
\lim\sup_{nâ†’âˆ} Î·_n â‰¤ q \lim\sup_{nâ†’âˆ} Î·_n
`.

