import VersoManual
import Docs.Papers

open Verso.Genre Manual
open Verso.Genre.Manual.InlineLean
open Verso.Code.External
open Docs

set_option pp.rawOnError true
set_option verso.exampleProject "../axioms_of_adaptivity"
set_option verso.exampleModule "AxiomsOfAdaptivity.Basics"
set_option maxHeartbeats 20000000

#doc (Manual) "Estimator Reduction" =>
%%%
htmlSplit := .never
tag := "estimator_reduction"
%%%

This chapter formalizes the proof of Lemma 4.7 from *AoA* which reads as

> *Lemma 4.7*: Stability (A1) and Reduction (A2) imply the estimator reduction
  $$`Î·(ğ“£_{â„“+1}; U(ğ“£_{â„“+1}))Â² â‰¤ Ï_{est} Î·(ğ“£_â„“; U(ğ“£_â„“))Â² + C_{est} ğ••[ğ“£_{â„“+1}; U(ğ“£_{â„“+1}), U(ğ“£_â„“)]Â²`
  for all $`â„“ âˆˆ â„•_0` with the constants $`0 < Ï_{est} < 1` and $`C_{est} > 0` which
  relate via
  $$`Ï_{est} = (1 + Î´)(1 - (1 - Ï_{red})Î¸) \quad \text{and} \quad C_{est} = C_{red} + (1 + Î´â»Â¹)C_{stab}Â²`
  for all sufficiently small $`Î´` such that $`Ï_{est} < 1`.

All the Lean code in this chapter is inside the `AdaptiveAlgorithm` namespace
so all definitions and theorems can be accessed on an instance of the
structure `AdaptiveAlgorithm` via dot notation. Also globally we introduce
the variable
```anchor alg
variable (alg : AdaptiveAlgorithm Î± Î²)
include alg
```

# Formal Statement
%%%
tag := "lemma47_formal_statement"
%%%

The wording "for all sufficiently small" hides the dependency
of the "constants" $`Ï_est` and $`C_est` on $`Î´`. For the formalized version we
define these values as functions of $`Î´` and show the estimator
reduction property for all $`Î´ > 0` such that $`Ï_{est}(Î´) < 1`.

We define the functions `Ï_est` and `C_est` as
```anchor lemma47_consts
def Ï_est Î´ := (1+Î´) * (1 - (1 - alg.Ï_red) * alg.Î¸)
noncomputable def C_est Î´ := alg.C_red + (1 + Î´â»Â¹) * alg.C_stab ^ 2
```

Then, the statement we want to prove is
```
âˆ€ Î´ > 0, (alg.Ï_est Î´ < 1) â†’
  âˆ€ l,
    alg.gÎ·2_seq (l + 1)
    â‰¤ alg.Ï_est Î´ * alg.gÎ·2_seq l
      + alg.C_est Î´ * alg.d (alg.ğ’¯ <| l + 1) (alg.U <| alg.ğ’¯ <| l+1) (alg.U <| alg.ğ’¯ <| l) ^ 2
```

# Utility lemmas

Before starting on the actual proof,
we show a few utility lemmata.

## DÃ¶rfler for refined elements

The first one is a DÃ¶rfler-type estimate for
the only the elements that have been refined:

> *Lemma (DÃ¶rfler for refined elements)*: For all $`l âˆˆ â„•_0` we have the
  estimate
  $$`
  Î¸ Î·^2(ğ’¯_{l}, U(ğ’¯_{l})) â‰¤ \sum_{t \in ğ’¯_l \setminus ğ’¯_{l+1}} Î·_{t}^2(ğ’¯_{l}, U(ğ’¯_{l}))
  `

The proof is straightforward, it follows from the DÃ¶rfler property,
$`â„³_l âŠ† ğ’¯_l \setminus ğ’¯_{l+1}` and that a sum does not increase when
we add non-negative summands. In Lean the proof reads as
```anchor doerfler_for_refined_elements
lemma doerfler_for_refined_elements :
    âˆ€ l, alg.Î¸ * gÎ·2_seq alg l
      â‰¤ âˆ‘ t âˆˆ (alg.ğ’¯ l \ alg.ğ’¯ (l+1)), alg.Î· (alg.ğ’¯ l) (alg.U <| alg.ğ’¯ l) t ^ 2 := by {
  intros l
  calc alg.Î¸ * gÎ·2_seq alg l
    _ â‰¤ âˆ‘ t âˆˆ alg.â„³ l, alg.Î· (alg.ğ’¯ l) (alg.U <| alg.ğ’¯ l) t ^ 2 := by exact (alg.hâ„³ l).2.1
    _ â‰¤ âˆ‘ t âˆˆ (alg.ğ’¯ l \ alg.ğ’¯ (l+1)), alg.Î· (alg.ğ’¯ l) (alg.U <| alg.ğ’¯ l) t ^ 2 := by {
      apply Finset.sum_le_sum_of_subset_of_nonneg
      Â· exact (alg.hâ„³ l).1
      Â· exact fun _ _ _ â†¦ sq_nonneg _
    }
}
```

## Estimate on Square of a Sum
Another purely analytical utility lemmas we are going to use is

> *Lemma (Square of Sum Estimate)*: For $`a,b â‰¥ 0` and $`Î´ > 0`
   $$`
   (a+b)^2 â‰¤ (1+Î´)a^2 + (1+Î´â»Â¹)b^2
   `
   holds

To show this we first need a generalized Young inequality

> *Lemma (Generalized Young inequality)*: For $`a,b â‰¥ 0`, $`Î´ > 0` and a HÃ¶lder-conjugate pair
  $`p,q` (meaning $`\frac1p = \frac1q`) the inequality
  $$`
  ab â‰¤ \frac{Î´}{p} a^p + \frac{1}{q Î´^(\frac{q}{p})} b^q
  ` holds.

We first prove the Young-type inequality by estimating
$$`
\begin{aligned}
ab &= ab (Î´^{1/p} Î´^{-1/p}) \\
&= (a Î´^{1/p}) (b Î´^{-1/p}) \\
&â‰¤ \frac{(a Î´^{1/p})^p}{p} + \frac{(b Î´^{-1/p})^q}{q} \\
&= \frac{Î´}{p} a^p + \frac{1}{q Î´^{q/p}} b^q
\end{aligned}
`
where we have used the regular Young inequality in step three.
This proof carries over to Lean using a `calc`-block:

```
lemma young_with_delta {a b Î´ p q : â„} (ha : 0 â‰¤ a)  (hb : 0 â‰¤ b) (hÎ´ : 0 < Î´) (hpq : p.HolderConjugate q): a*b â‰¤ Î´/p * a^p + 1/(q*Î´^(q/p)) * b^q := by {
  have hÎ´â‚‚ := le_of_lt hÎ´
  have hpow_nonneg := Real.rpow_nonneg hÎ´â‚‚

  calc a*b
    _ = a * b * (Î´ ^ pâ»Â¹ * (Î´ ^ pâ»Â¹)â»Â¹) := by field_simp
    _ = a * Î´ ^ (1 / p) * (b * 1 / Î´ ^ (1 / p)) := by ring_nf
    _ â‰¤ (a * Î´ ^ (1 / p)) ^ p / p + (b * 1 / Î´ ^ (1 / p)) ^ q / q := by {
      apply Real.young_inequality_of_nonneg _ _ hpq
      Â· exact mul_nonneg ha (hpow_nonneg _)
      Â· apply mul_nonneg <;> simp [hb, ha, hpow_nonneg]
    }
    _ = Î´/p * a^p + (b * 1 / Î´ ^ (1 / p)) ^ q / q := by {
      rw [Real.mul_rpow ha <| hpow_nonneg _, â†Real.rpow_mul hÎ´â‚‚]
      simp [inv_mul_cancelâ‚€ <| Real.HolderTriple.ne_zero hpq, mul_comm]
      ring
    }
    _ = Î´/p * a^p + 1/(q*Î´^(q/p)) * b^q := by {
      field_simp
      rw [Real.div_rpow hb <| hpow_nonneg _, â†Real.rpow_mul hÎ´â‚‚]
      ring_nf
    }
}
```

Now we can show the estimate on the square of a sum again by
doing a calculation involving the generalized Young equation
with $`p=q=\frac12`.
$$`
\begin{aligned}
(a+b)^2 &= a^2 + 2ab + b^2 \\
&â‰¤ a^2 + 2 (\frac{Î´}{2} a^2 + \frac{1}{2Î´} b^2) + b^2 \\
&= (1+Î´)a^2 + (1+Î´â»Â¹)b^2
\end{aligned}
`

This is also straightforward to show this way in Lean
```
lemma sum_square_le_square_sum {a b : â„} (ha : 0 â‰¤ a) (hb : 0 â‰¤ b) :
    âˆ€ Î´ > 0, (a+b)^2 â‰¤ (1+Î´)*a^2 + (1+Î´â»Â¹)*b^2 := by {
  intros Î´ hÎ´
  have := young_with_delta ha hb hÎ´ Real.HolderConjugate.two_two
  calc (a + b) ^ 2
    _ = a^2 + 2*(a*b) + b^2 := by ring
    _ â‰¤ a^2 + 2*(Î´/2 * a^2 + 1/(2*Î´) * b^2) + b^2 := by simpa using this
    _ = (1+Î´)*a^2 + (1+Î´â»Â¹)*b^2 := by ring
}
```

## Distance Estimate

The last utility lemma we will show is that for $`a,b,c âˆˆ â„`, $`a â‰¥ 0` the
condition $`|a-b| â‰¤ c` implies $`a^2 â‰¤ (b+c)^2`.

To show this we note that the condition especially implies $`a-b â‰¤ c`,
which means $`a â‰¤ b + c` by adding $`b`. Because by assumption $`a â‰¥ 0`
and we can take the square and arrive at the desired result. The Lean version
of this proof is
```
lemma square_estimate_of_small_distance {a b c : â„} (ha : 0 â‰¤ a) (h : |a-b| â‰¤ c) :
  a^2 â‰¤ (b+c)^2 := by {
  have : a - b â‰¤ c := le_of_max_le_left h
  have : a â‰¤ b + c := tsub_le_iff_left.mp this
  exact pow_le_pow_leftâ‚€ ha this 2
}
```

# Proof of Estimator Reduction

For the main proof of lemma 4.7,
we begin by introducing {anchorTerm estimator_reduction_1}`Î´` and
{anchorTerm estimator_reduction_1}`l` along with the assumptions
that `Î´ > 0` and `alg.Ï_est Î´ < 1`. We also define abbreviations for
terms that appear in the proof and are lenghty to write.

```anchor estimator_reduction_1
theorem estimator_reduction : âˆ€ Î´ > 0, (alg.Ï_est Î´ < 1) â†’
    âˆ€ l, alg.gÎ·2_seq (l + 1)
         â‰¤ alg.Ï_est Î´ * alg.gÎ·2_seq l
           + alg.C_est Î´ * alg.d (alg.ğ’¯ <| l + 1) (alg.U <| alg.ğ’¯ <| l+1) (alg.U <| alg.ğ’¯ <| l) ^ 2 := by {
  intros Î´ hÎ´ hÏ_est l

  let summand n t := alg.Î· (alg.ğ’¯ n) (alg.U <| alg.ğ’¯ <| n) t ^ 2
  let distance n := alg.d (alg.ğ’¯ <| n + 1) (alg.U <| alg.ğ’¯ <| n + 1) (alg.U <| alg.ğ’¯ <| n) ^ 2
```

Then the estimate can be shown in one long chain of equalities and estimates,
starting from $`Î·^2(ğ’¯_{l+1}, U(ğ’¯_{l+1}))`, where every step has a Lean proof
of reasonable size. We will present the Lean proof interlaced with the mathematical
explanation of the current calculation step.

We start with
$$`
\begin{aligned}
& Î·^2(ğ’¯_{l+1}, U(ğ’¯_{l+1})) \\
&= \sum_{t \in ğ’¯_{l+1} \setminus ğ’¯_l} Î·_t^2(ğ’¯_{l+1}, U(ğ’¯_{l+1})) + \sum_{t \in ğ’¯_l \cap ğ’¯_{l+1}} Î·_t^2(ğ’¯_{l+1}, U(ğ’¯_{l+1}))
\end{aligned}
`
which hold by the definition of the global error and basic set identities. In Lean
we essentially use the {anchorTerm estimator_reduction_2}`sum_union` theorem from
mathlib:

```anchor estimator_reduction_2
  calc gÎ·2_seq alg (l + 1)
    _ = âˆ‘ t âˆˆ alg.ğ’¯ (l + 1) \ alg.ğ’¯ l, summand (l+1) t
        + âˆ‘ t âˆˆ alg.ğ’¯ l âˆ© alg.ğ’¯ (l+1), summand (l+1) t := by {
      unfold gÎ·2_seq gÎ·2
      have h_eq : (alg.ğ’¯ (l + 1)).val = (â†‘(alg.ğ’¯ (l + 1)) \ â†‘(alg.ğ’¯ l)) âˆª (â†‘(alg.ğ’¯ (l + 1)) âˆ© â†‘(alg.ğ’¯ l)) := by {
        exact Eq.symm (sdiff_union_inter _ _)
      }
      nth_rw 1 [h_eq]
      simp [sum_union (disjoint_sdiff_inter _ _)]
      nth_rw 1 [inter_comm]
    }
```

Next, we apply the reduction property on refined elements (A2) to reach

$$`
\begin{aligned}
&\le Ï_{red} \sum_{t \in ğ’¯_l \setminus ğ’¯_{l+1}} Î·_t^2(ğ’¯_l, U(ğ’¯_l)) + C_{red} ğ••[ğ’¯_{l+1}, U(ğ’¯_{l+1}), U(ğ’¯_l)]^2 \\
&\quad + \sum_{t \in ğ’¯_l \cap ğ’¯_{l+1}} Î·_t^2(ğ’¯_{l+1}, U(ğ’¯_{l+1})).
\end{aligned}
`

In Lean we can see that this is really a direct application of the axiom:
```anchor estimator_reduction_3
    _ â‰¤ alg.Ï_red * âˆ‘ t âˆˆ alg.ğ’¯ l \ alg.ğ’¯ (l + 1), summand l t
        + alg.C_red * distance l
        + (âˆ‘ t âˆˆ alg.ğ’¯ l âˆ© alg.ğ’¯ (l + 1), summand (l + 1) t) := by
      rel[alg.a2 (alg.ğ’¯ l) (alg.ğ’¯ <| l + 1) (alg.hğ’¯ l)]
```
We use the advanced tactic `rel` here which can automatically find
proofs for inqualities when a nested term is estimated. Often, it finds
proofs for non-negativity on its own, which is necessary for upper bounds
on products. E.g. when we have `h : a â‰¤ b` and successfully show `C*a â‰¤ C*b` using
`rel [h]` then the tactic had to account for the non-negativity of C.

Now, in one step we can estimate

$$`
\begin{aligned}
&\le Ï_{red} \sum_{t \in ğ’¯_l \setminus ğ’¯_{l+1}} Î·_t^2(ğ’¯_l, U(ğ’¯_l)) + C_{red} ğ••[ğ’¯_{l+1}, U(ğ’¯_{l+1}), U(ğ’¯_l)]^2 \\
&\quad + (1+Î´) \sum_{t \in ğ’¯_l \cap ğ’¯_{l+1}} Î·_t^2(ğ’¯_l, U(ğ’¯_l)) + (1+Î´â»Â¹) C_{stab}^2 ğ••[ğ’¯_{l+1}, U(ğ’¯_{l+1}), U(ğ’¯_l)]^2
\end{aligned}
`
by combining stability on non-refined element domains (A1) and the
two utility lemmas from above. The Lean proof for this step reads as

```anchor estimator_reduction_4
    _ â‰¤ alg.Ï_red * âˆ‘ t âˆˆ alg.ğ’¯ l \ alg.ğ’¯ (l + 1), summand l t
        + alg.C_red * distance l
        + ((1 + Î´) * âˆ‘ t âˆˆ alg.ğ’¯ l âˆ© alg.ğ’¯ (l + 1), summand l t
        + (1 + Î´â»Â¹) * (alg.C_stab ^ 2 * distance l)) := by {
      have := alg.a1
        (alg.ğ’¯ l)
        (alg.ğ’¯ <| l + 1)
        (alg.hğ’¯ l)
        (alg.ğ’¯ l âˆ© alg.ğ’¯ (l + 1))
        (fun _ a â†¦ a)
        (alg.U <| alg.ğ’¯ <| l)
        (alg.U <| alg.ğ’¯ <| l + 1)
      have := square_estimate_of_small_distance (Real.sqrt_nonneg _) this
      have hâ‚ : 0 â‰¤ alg.C_stab * alg.d (alg.ğ’¯ (l + 1)) (alg.U (alg.ğ’¯ (l + 1))) (alg.U (alg.ğ’¯ l)) := by {
        apply mul_nonneg (le_of_lt alg.hC_stab)
        apply alg.non_neg
      }
      have := le_trans this <| sum_square_le_square_sum (Real.sqrt_nonneg _) hâ‚ Î´ hÎ´

      rw [Real.sq_sqrt, Real.sq_sqrt, mul_pow] at this
      Â· change âˆ‘ t âˆˆ alg.ğ’¯ l âˆ© alg.ğ’¯ (l + 1), summand (l + 1) t
          â‰¤ (1 + Î´) * âˆ‘ t âˆˆ alg.ğ’¯ l âˆ© alg.ğ’¯ (l + 1), summand l t
          + (1 + Î´â»Â¹) * (alg.C_stab ^ 2 * distance l) at this
        rel [this]
      all_goals apply_rules [sum_nonneg', fun _ â†¦ sq_nonneg _]
    }
```
Here we use the `change` tactic in order to switch to an equivalent type for hypotheses
{anchorTerm estimator_reduction_4}`this` in order for the `rel` tactic to suceed in
closing one of the three goals.

Then we rewrite
$$`
\begin{aligned}
&= Ï_{red} \sum_{t \in ğ’¯_l \setminus ğ’¯_{l+1}} Î·_t^2(ğ’¯_l, U(ğ’¯_l)) + (1+Î´) \sum_{t \in ğ’¯_l \cap ğ’¯_{l+1}} Î·_t^2(ğ’¯_l, U(ğ’¯_l)) \\
&\quad + (C_{red} + (1+Î´â»Â¹) C_{stab}^2) ğ••[ğ’¯_{l+1}, U(ğ’¯_{l+1}), U(ğ’¯_l)]^2
\end{aligned}
`
by basic algebra. Lean can prove this on its own using the `ring` tactic:

```anchor estimator_reduction_5
    _ = alg.Ï_red * âˆ‘ t âˆˆ alg.ğ’¯ l \ alg.ğ’¯ (l+1), summand l t
        + (1+Î´) * âˆ‘ t âˆˆ alg.ğ’¯ l âˆ© alg.ğ’¯ (l+1), summand l t
        + (alg.C_red + (1 + Î´â»Â¹) * alg.C_stab ^ 2) * distance l := by ring
```

Next, by definition of the global error $`Î·^2` and basic set identities
$$`
\begin{aligned}
&= Ï_{red} \sum_{t \in ğ’¯_l \setminus ğ’¯_{l+1}} Î·_t^2(ğ’¯_l, U(ğ’¯_l)) \\
&\quad + (1+Î´) \left(Î·^2(ğ’¯_l, U(ğ’¯_l)) - \sum_{t \in ğ’¯_l \setminus ğ’¯_{l+1}} Î·_t^2(ğ’¯_l, U(ğ’¯_l))\right).
\end{aligned}
`

The Lean proof is similar to the first step we did:
```anchor estimator_reduction_6
    _ = alg.Ï_red * âˆ‘ t âˆˆ alg.ğ’¯ l \ alg.ğ’¯ (l+1), summand l t
        + (1+Î´) * (gÎ·2_seq alg l -  âˆ‘ t âˆˆ alg.ğ’¯ l \ alg.ğ’¯ (l+1), summand l t)
        + (alg.C_red + (1 + Î´â»Â¹) * alg.C_stab ^ 2) * distance l := by {
      congr
      have h_eq : (alg.ğ’¯ l).val = (â†‘(alg.ğ’¯ l) \ â†‘(alg.ğ’¯ (l + 1))) âˆª (â†‘(alg.ğ’¯ l) âˆ© â†‘(alg.ğ’¯ (l+1))) := by exact Eq.symm (sdiff_union_inter _ _)
      have h_dis: Disjoint ((alg.ğ’¯ l : Finset Î±) \ alg.ğ’¯ (l + 1)) (alg.ğ’¯ l âˆ© alg.ğ’¯ (l+1)) := by {
        exact disjoint_sdiff_inter _ _
      }
      unfold gÎ·2_seq gÎ·2
      nth_rw 2 [h_eq]
      rw [sum_union (disjoint_sdiff_inter _  _)]
      ring
    }
```
The essential tool here
is {anchorTerm estimator_reduction_6}`sum_union` from mathlib. Also
note the use of the `gcongr` tactic which can simplify congruences in
proof goals. It has the same capabilities as `rel` but does not
try to close a goal. Given the current goal is an inequality,
it rather tries to find as much common terms on both sides as possible
and leaves the inequality between the terms that differ open as a new goal, filling
in the proof to go from the simpler inequality to the original goal on its own.

Now, because $`Î´ > 0` we have
$$`
\begin{aligned}
&\quad + (C_{red} + (1+Î´â»Â¹) C_{stab}^2) ğ••[ğ’¯_{l+1}, U(ğ’¯_{l+1}), U(ğ’¯_l)]^2 \\
&\le (1+Î´) Ï_{red} \sum_{t \in ğ’¯_l \setminus ğ’¯_{l+1}} Î·_t^2(ğ’¯_l, U(ğ’¯_l)).
\end{aligned}
`

This is done rather easily in Lean using `gcongr` again:
```anchor estimator_reduction_7
    _ â‰¤ (1+Î´) * alg.Ï_red * âˆ‘ t âˆˆ alg.ğ’¯ l \ alg.ğ’¯ (l+1), summand l t
        + (1+Î´) * (gÎ·2_seq alg l - âˆ‘ t âˆˆ alg.ğ’¯ l \ alg.ğ’¯ (l+1), summand l t)
        + (alg.C_red + (1 + Î´â»Â¹) * alg.C_stab ^ 2) * distance l := by {
      gcongr
      refine (le_mul_iff_one_le_left ?_).mpr ?_
      Â· exact alg.hÏ_red.1
      Â· linarith
    }
```

The last steps are basic algebra and one application of the
Doerfler marking for refined elements lemma.
$$`
\begin{aligned}
&\quad + (1+Î´) \left(Î·^2(ğ’¯_l, U(ğ’¯_l)) - \sum_{t \in ğ’¯_l \setminus ğ’¯_{l+1}} Î·_t^2(ğ’¯_l, U(ğ’¯_l))\right) \\
&\quad + (C_{red} + (1+Î´â»Â¹) C_{stab}^2) ğ••[ğ’¯_{l+1}, U(ğ’¯_{l+1}), U(ğ’¯_l)]^2 \\
&= (1+Î´) \left(Î·^2(ğ’¯_l, U(ğ’¯_l)) - (1 - Ï_{red}) \sum_{t \in ğ’¯_l \setminus ğ’¯_{l+1}} Î·_t^2(ğ’¯_l, U(ğ’¯_l))\right) \\
&\quad + (C_{red} + (1+Î´â»Â¹) C_{stab}^2) ğ••[ğ’¯_{l+1}, U(ğ’¯_{l+1}), U(ğ’¯_l)]^2 \\
&\le (1+Î´) (Î·^2(ğ’¯_l, U(ğ’¯_l)) - (1 - Ï_{red}) Î¸ Î·^2(ğ’¯_l, U(ğ’¯_l))) \\
&\quad + (C_{red} + (1+Î´â»Â¹) C_{stab}^2) ğ••[ğ’¯_{l+1}, U(ğ’¯_{l+1}), U(ğ’¯_l)]^2 \\
&= (1+Î´) (1 - (1 - Ï_{red}) Î¸) Î·^2(ğ’¯_l, U(ğ’¯_l)) \\
&\quad + (C_{red} + (1+Î´â»Â¹) C_{stab}^2) ğ••[ğ’¯_{l+1}, U(ğ’¯_{l+1}), U(ğ’¯_l)]^2
\end{aligned}
`
This finished the proof as we have arrived at the upper bound we wanted to have.

It carries over to Lean very nicely:
```anchor estimator_reduction_8
    _ = (1+Î´) * (gÎ·2_seq alg l - (1-alg.Ï_red) * âˆ‘ t âˆˆ alg.ğ’¯ l \ alg.ğ’¯ (l+1), summand l t)
        + (alg.C_red + (1 + Î´â»Â¹) * alg.C_stab ^ 2) * distance l := by ring
    _ â‰¤ (1+Î´) * (gÎ·2_seq alg l - (1-alg.Ï_red) * (alg.Î¸ * gÎ·2_seq alg l))
        + (alg.C_red + (1 + Î´â»Â¹) * alg.C_stab ^ 2) * distance l := by {
      have hâ‚ : 0 â‰¤ 1 - alg.Ï_red := sub_nonneg_of_le <| le_of_lt alg.hÏ_red.2
      rel[alg.doerfler_for_refined_elements l, hâ‚]
    }
    _ = (1+Î´) * (1 - (1-alg.Ï_red) * alg.Î¸) * gÎ·2_seq alg l
        + (alg.C_red + (1 + Î´â»Â¹) * alg.C_stab ^ 2) * distance l := by ring
```
Now all Lean goals are closed and the theorem is proven.
